### 1. 限流
[redis+lua限流](https://segmentfault.com/a/1190000019477128)
```
前言
限流器产生的原因
在设计大规模高并发系统的时候，我们要考虑设计的系统能扛住多大的流量，突发引起雪崩怎么办，面对这些问题，这时候我们常常用保护系统的三大利器：缓存、限流、服务降级。但是这三种利器适用的场景不同。例如：缓存的目的是提升系统的访问速度，提高系统的处理能力，在尽可能短的时间内完成用户的请求。降级是出现调用服务出现超时等问题的时候，避免影响主要核心流程,暂时把出现问题的模块屏蔽掉，等待问题处理后再恢复。但是有一些场景是不能通过缓存和降级来解决的。比如秒杀，春节抽奖等活动会出现瞬间大流量的情况，这种情况下会出现资源耗尽的情况，此时限流是一种主动调整流量输出速率的措施，当请求过多的时候可以采取直接拒绝服务或者阻塞的形式来应对。限流的目的是为了保护每个服务节点，保护服务器背后资源（如数据库，缓存，调用的服务），避免某个服务影响其他使用方。

限流器常用算法
单机限流器
从算法层面：常见的算法有计数器、滑动窗口、队列、令牌桶、漏桶等算法，这些算法已经很成熟，可以直接使用，但是单机限流器在分布式系统中存在一些问题，在分布式系统中采用的数据库和redis等组件都是集中式的，存在多对一的关系，如果单纯以单机的性能为指标作为限流的依据，很显然极有可能将数据库打挂，但是如果以数据库的单位时间内请求量为依据均衡到每台机器上，此时会出现还没达到数据库和机器的瓶颈已经将一部分请求拒绝掉。而分布式限流器能很好的解决这些出现的问题，使系统在健康的情况下 发挥最大的能效。

分布式限流器
分布式限流器目前主流的方式是通过redis和zookeeper的方式来解决，然而zookeeper的实现和操作比较复杂，开发接入维护成本高，一般采用的不多。绝大多数采用的是redis方式，redis中采用redis+lua结合的方式比较多，对于不熟悉lua脚本的同学使用维护成本很高，针对这些问题我们设计开发并持续优化了一个go语言版本的限流器，借助于redis实现。

分布式限流器原理
该分布式限流器由最初的设计到应用并经过一系列的优化演进，目前支持多种场景的限流方式，包括： 1.单机限流 2.分布式精准限流 3.分布式超大qps限流 4.分布式+单机双结合模式限流

单机限流器
令牌桶算法是使用较广泛的单机限流方式，它是通过令牌桶中是否存在令牌来指示何时可以发送流量。如果令牌桶中存在令牌则允许发送流量；而如果令牌桶中不存在令牌，则不允许发送流量。因此，如果突发门限被合理地配置并且桶中有足够的令牌，那么流量可以以峰值速率发送信息。



核心代码实现

1.令牌桶初始化

func (s *LocalLimiter) SetLimit(newLimit int64) (old int64) {
//...
s.bucket = ratelimit.NewBucket(time.Second/time.Duration(newLimit), newLimit)
}
2.通过canPass方法去获取令牌判断是否限流

func (s *LocalLimiter) CanPass() (ret bool) {
   //...
   return s.bucket.TakeAvailable(1) == 1
}

分布式精准限流器
分布式精准限流模式和单机限流模式原理类似，只是令牌桶换成了redis桶去控制，因为redis天然支持分布式处理，所有业务每次请求都需要去redis桶中去获取令牌，当在指定的时间内redis中的令牌被获取完毕之后开始拒绝请求达到限流的目的。 它的使用场景是单机限流无法满足要求需要分布式精准去限制流量的大小且流量不是非常大的场景。

核心代码实现

1.redis桶结构

type SimpleLimiter struct {
   key             string  
   limit           int64
   redisClient     *goredis.Client
   backRedisClient *goredis.Client
   metricsClientV2 *metrics.MetricsClientV2
}
key:代表限流的接口名称，limit表示在单位时间限流的大小， redisClient和backRedisClient代表主备两个redis结构，metricsClientV2:在限流执行的过程中需要通过metrics采集数据将监测数据时时显示出来。 2.redis桶初始化

func NewSimpleLimiter(metricsClientV2 *metrics.MetricsClientV2, serviceName string, configJson *simplejson.Json, key string, limit int64, redisClient *goredis.Client, backRedisClient *goredis.Client) *SimpleLimiter {
   initTccClientAndConfKey(configJson, serviceName)
   return &SimpleLimiter{
      key:             "simple-limiter-" + key,
      limit:           limit,
      redisClient:     redisClient,
      backRedisClient: backRedisClient,
      metricsClientV2: metricsClientV2,
   }
}
3.通过canPass方法去获取令牌判断是否限流

func (s *SimpleLimiter) CanPass() bool {
   val := getRedisClient(s.redisClient, s.backRedisClient).Incr(s.key).Val()
   if val == 1 {
      getRedisClient(s.redisClient, s.backRedisClient).PExpire(s.key, time.Second)
      logs.Debug("SimpleLimiter start new counter")
   }
   dur := getRedisClient(s.redisClient, s.backRedisClient).PTTL(s.key).Val()
   if dur < 0 {
      // 没有key或未设置过期时间
      getRedisClient(s.redisClient, s.backRedisClient).PExpire(s.key, time.Second)
   }
   return val <= s.limit
}
分布式大qps限流
分布式精准限流对qps不是太大的时候效果很好，但是对于像春节这种大型的活动，几万甚至几十万qps的服务，精准限流器无法到达要求，就需要采用一种能比精准限流更强且限流值不是100%的限流器。该限流器与精准限流器最大的区别是加入了step(步长)的概念，限流接口的令牌数量仍然是在reids中存放着，但是取令牌数量时候不像精准限流器那样一次取一个而是以步长为单位去获取，每次获取单位步长的令牌到本地缓存，新的请求进入需要先去本地的令牌桶中去获取令牌，如果本地令牌桶中消耗完之后需要到redis桶中再次去获取直至redis桶中的令牌被消耗完毕。它比精准限流器在相同的限制数量上减少了去redis请求的次数，相当于依靠远端redis+本地结合的方式可以支持高并发大流量的限制。



1.redis桶结构

type CacheLimiter struct {
   key             string
   limit           int64
   step            int64
   stock           int64     // 当前库存，大于0直接减一并返回true
   banEndTime      time.Time // 该时间以前均不能Pass
   mutex           sync.Mutex
   redisClient     *goredis.Client
   backRedisClient *goredis.Client
   metricsClientV2 *metrics.MetricsClientV2
}
2.刷新处理库存核心代码

func (s *CacheLimiter) refresh() {
 if s.stock > 0 || s.isBanned() {
     //...
   val, err := getRedisClient(s.redisClient, s.backRedisClient).IncrBy(s.key, s.step).Result()
   if err != nil {
      logs.Error("CacheLimiter IncrBy err: %v, key: %s", err, s.key)
      s.banEndTime = time.Now().Add(time.Second)
      return
   }
   if val <= 0 {
     //...
      return
   }

   if val == s.step {
      getRedisClient(s.redisClient, s.backRedisClient).PExpire(s.key, time.Second)
   }
   if val <= s.limit {
      s.stock += s.step
      return
   } else if val < s.limit+s.step {
      s.stock += (s.limit - (val - s.step))
      return
   }

dur, err := getRedisClient(s.redisClient, s.backRedisClient).PTTL(s.key).Result()
   //...
   if dur < 0 || dur > time.Second {
      if dur == time.Millisecond * -2 {
         s.banEndTime = time.Now()
         return
      }
      getRedisClient(s.redisClient, s.backRedisClient).PExpire(s.key, time.Second)
      dur = time.Second
   }
   s.banEndTime = time.Now().Add(dur)
   return
}
3.通过canPass方法去获取令牌判断是否限流

func (s *CacheLimiter) CanPass() (ret bool) {
   s.mutex.Lock()
   defer s.mutex.Unlock()
   if s.stock > 0 {
      s.stock--
      return true
   }
   if s.isBanned() {
      return false
   }
   s.refresh()
   if s.stock > 0 {
      s.stock--
      return true
   }
   return false
}
分布式大qps+单机限流
分布式限流+单机限流模式采用tcc将二者结合起来，当二者都开启的时候优先使用分布式大qps限流器，该限流器优先采用主redis,当主redis出现问题的时候可以热切到备redis,当备redis出现问题的时候还可以切到单机限流器，保证限流器正常运行。

tcc 配置开关结构

type TCCSwitchConf struct {
   RedisSwitch bool `json:"redis_switch"`
   IsUseLocalLimiter bool `json:"is_use_local_limiter"`
}
分布式限流器的演进
该分布式限流器经过一系列的迭代过程且经过多个业务线的检验，运行很稳定。



分布式限流器使用
使用方式：该限流器和kite框架进行完整的融合，引入使用很简单。

配置接口文档信息，配置两套：conf启动文件配置一套 +tcc配置一套 配置两套的原因：tcc配置可以随时根据系统的实际情况对限流器的参数进行调整，添加本地conf可以初始化限流器同时在启动时刻tcc出现异常的时候仍然能读取配置正常限流。
conf和tcc配置示例文件格式如下：

redis_qps_limit": {
   "qps_limit": {
     "QueryCouponBatches": {
        "qps": 1000,
        "step": 50
},
     "QueryCouponsByUID": {
       "qps": 2000,
       "step": 100
  },
 }
}
QueryCouponBatches:代表限流的rpc接口名称，qps代表每秒限制的总数量，step:代表每一次从redis中获取的指定的数量。

初始化接口拦截器
func BuildOverLoadMap() map[string]*OverloadInfo {
    var overloadMap = map[string]*OverloadInfo{
     "QueryCouponBatches": {
      qps: 0,
      responseFactory: func() endpoint.KiteResponse {
      return &KiteCouponBatchQueryResponse{
      &coupon.CouponBatchQueryResponse{
      BaseResp: &overloadBaseResp,
     },
    }
   }
  }
return overloadMap
 } 
}
3.添加拦截器组件 当rpc请求进入时需要先经过kite框架一系列拦截器，经过限流器拦截器会去判断该请求是否限流，在kite框架中添加该组件拦截器。

limiter.AddOverLoadMW(GetMetricV2Client(), config.ConfigJson, buildOverLoadMap(), model.RedisClient, model.BackRedisClient)
核心拦截逻辑如下：

if overloadInfo, ok := overloadMap[method]; ok {
   // 只有有limiter的才是需要限流的
   if overloadInfo.limiter != nil && 
    overloadInfo.limiter.CanPass() == false {
       resp := overloadInfo.responseFactory()
       return resp, nil
   } else {
    }
    }
    return next(ctx, request)
   }
 }
kite.Use(overloadMW)
分布式限流器使用效果
该限流器在财经的各个部门得到推广和实践，春节活动的时候支付网关、会员系统、支付核心、渠道、支付营销等部门使用效果良好。营销组春节转盘活动线上使用效果，限流接口数据统计信息详情：



分布式限流器优缺点


总结
1.go语言版本限流器并和kite框架进行很好融合（采用kite框架拦截器的方式）。 2.支持多种限流方式：单机限流、分布式精准限流、分布式大qps限流，分布式+单机限流模式。 3.为了保证redis的可靠性，支持主备两个redis使用，当主redis出现异常的情况下可以切备redis。 4.采用conf文件+tcc双配置文件的方式，能确保在任意一方配置出现问题的时候，限流器可以正常启动运行。 5.该限流器采用独立的项目模块开发，拥有完善的日志监控配置，接入方式简单，易用性很强。 6.该限流器已在财经各部门使用并且得到了春节活动的考验，效果良好。
```